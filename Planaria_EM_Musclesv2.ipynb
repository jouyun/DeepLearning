{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic Unet for Fiji "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy.optimize import curve_fit\n",
    "import skimage\n",
    "from PIL import Image\n",
    "import random\n",
    "import utils\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data='/home/ubuntu/Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_scaler=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=4.8E10\n",
    "std=4.48E9\n",
    "x_dim=512\n",
    "y_dim=512\n",
    "channels=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "both=skimage.external.tifffile.imread(data+'Training_RotShift.tif')\n",
    "both=np.swapaxes(np.swapaxes(both,1,2), 2,3)\n",
    "\n",
    "#subsetA=[j for i in range(0,100) for j in range(i*54,i*54+9)]+[j for i in range(0,100) for j in range(27+i*54,27+i*54+9)]\n",
    "#subsetB=[j for i in range(0,100) for j in range(9+i*54,9+i*54+9)]+[j for i in range(0,100) for j in range(9+27+i*54,9+27+i*54+9)]\n",
    "#subsetC=[j for i in range(0,100) for j in range(18+i*54,18+i*54+9)]+[j for i in range(0,100) for j in range(18+27+i*54,18+27+i*54+9)]\n",
    "#subset=subsetA+subsetB+subsetC\n",
    "\n",
    "#both=both[subset,:,:,:]\n",
    "\n",
    "x_dim=both.shape[1]\n",
    "y_dim=both.shape[2]\n",
    "channels=both.shape[3]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=both[:,:,:,0:-1]\n",
    "train_truth=both[:,:,:,[-1]]\n",
    "\n",
    "both=0\n",
    "\n",
    "#non_zeros=np.where(train_data!=0)\n",
    "#mean=np.mean(train_data)\n",
    "#std=np.std(train_data)\n",
    "mean=4.8E10\n",
    "std=4.48E9\n",
    "\n",
    "\n",
    "train_data=(train_data-mean)/(std*1)+0.5\n",
    "train_truth[np.where(train_truth>0.1)]=1\n",
    "train_truth[np.where(train_truth<0)]=0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation=skimage.external.tifffile.imread(data+'Validation_annotated.tif')\n",
    "validation=np.swapaxes(np.swapaxes(validation,1,2), 2,3)\n",
    "\n",
    "validation_data=validation[:,:,:,0:-1]\n",
    "validation_data=(validation_data-mean)/(std*1)+0.5\n",
    "\n",
    "validation_truth=validation[:,:,:,[-1]]\n",
    "validation_truth[np.where(validation_truth>0.1)]=1\n",
    "validation_truth[np.where(validation_truth<0)]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48000000000.0, 4480000000.0]\n"
     ]
    }
   ],
   "source": [
    "print([mean, std])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to create a convolutional layer, including conv, relu, maxpool, that can be called multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "#Input and output\n",
    "x=tf.placeholder(dtype=tf.float32, shape=[None, x_dim,y_dim,channels], name='x')\n",
    "y=tf.placeholder(dtype=tf.float32, shape=[None, x_dim,y_dim,1], name='y')\n",
    "lr=tf.placeholder(dtype=tf.float32, shape=[], name='learning_rate')\n",
    "dr=tf.placeholder(dtype=tf.float32, shape=[], name='dropout_rate')\n",
    "\n",
    "xr=tf.identity(x)\n",
    "yr=tf.identity(y)\n",
    "\n",
    "#base_scaler=32\n",
    "\n",
    "#Going down\n",
    "A1=tf.keras.layers.Conv2D(base_scaler, 5, padding='SAME', activation=utils.leaky_relu)(xr)\n",
    "A2=tf.keras.layers.Conv2D(base_scaler, 3, padding='SAME', activation=utils.leaky_relu)(A1)\n",
    "\n",
    "B0=tf.nn.max_pool(A2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "B1=tf.keras.layers.Conv2D(2*base_scaler, 3, padding='SAME', activation=utils.leaky_relu)(B0)\n",
    "B2=tf.keras.layers.Conv2D(2*base_scaler, 3, padding='SAME', activation=utils.leaky_relu)(B1)\n",
    "\n",
    "C0=tf.nn.max_pool(B2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "C1=tf.keras.layers.Conv2D(4*base_scaler, 3, padding='SAME', activation=utils.leaky_relu)(C0)\n",
    "C2=tf.keras.layers.Conv2D(4*base_scaler, 3, padding='SAME', activation=utils.leaky_relu)(C1)\n",
    "\n",
    "D0=tf.nn.max_pool(C2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "D1=tf.keras.layers.Conv2D(8*base_scaler, 3, padding='SAME', activation=utils.leaky_relu)(D0)\n",
    "D2=tf.keras.layers.Conv2D(8*base_scaler, 3, padding='SAME', activation=utils.leaky_relu)(D1)\n",
    "\n",
    "E0=tf.nn.max_pool(D2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "E1=tf.keras.layers.Conv2D(16*base_scaler, 3, padding='SAME', activation=utils.leaky_relu)(E0)\n",
    "E2=tf.keras.layers.Conv2D(16*base_scaler, 3, padding='SAME', activation=utils.leaky_relu)(E1)\n",
    "\n",
    "#Coming up\n",
    "DD0=tf.keras.layers.Conv2DTranspose(8*base_scaler, kernel_size=3, strides=2, padding='SAME')(E2)\n",
    "DD1=tf.concat(axis=3, values=[DD0,D2])\n",
    "DD2=tf.keras.layers.Conv2D(8*base_scaler, 3, padding='SAME', activation=utils.leaky_relu)(DD1)\n",
    "DD3=tf.keras.layers.Conv2D(8*base_scaler, 3, padding='SAME', activation=utils.leaky_relu)(DD2)\n",
    "\n",
    "CC0=tf.keras.layers.Conv2DTranspose(4*base_scaler, kernel_size=3, strides=2, padding='SAME')(DD3)\n",
    "CC1=tf.concat(axis=3, values=[CC0,C2])\n",
    "CC2=tf.keras.layers.Conv2D(4*base_scaler, 3, padding='SAME', activation=utils.leaky_relu)(CC1)\n",
    "CC3=tf.keras.layers.Conv2D(4*base_scaler, 3, padding='SAME', activation=utils.leaky_relu)(CC2)\n",
    "\n",
    "BB0=tf.keras.layers.Conv2DTranspose(2*base_scaler, kernel_size=3, strides=2, padding='SAME')(CC3)\n",
    "BB1=tf.concat(axis=3, values=[BB0,B2])\n",
    "BB2=tf.keras.layers.Conv2D(2*base_scaler, 3, padding='SAME', activation=utils.leaky_relu)(BB1)\n",
    "BB3=tf.keras.layers.Conv2D(2*base_scaler, 3, padding='SAME', activation=utils.leaky_relu)(BB2)\n",
    "\n",
    "AA0=tf.keras.layers.Conv2DTranspose(base_scaler, kernel_size=3, strides=2, padding='SAME')(BB3)\n",
    "AA1=tf.concat(axis=3, values=[AA0,A2])\n",
    "AA2=tf.keras.layers.Conv2D(base_scaler, 3, padding='SAME', activation=utils.leaky_relu)(AA1)\n",
    "AA3=tf.keras.layers.Conv2D(base_scaler, 3, padding='SAME', activation=utils.leaky_relu)(AA2)\n",
    "\n",
    "logits=tf.keras.layers.Conv2D(1, 1, padding='SAME', activation=utils.leaky_relu)(AA3)\n",
    "probs=tf.tanh(logits, name='probabilities')\n",
    "\n",
    "diff=tf.subtract(probs, yr)\n",
    "LSQ=tf.multiply(diff,diff)\n",
    "#Added this to make the outlines more potent in error function\n",
    "#OutError, MaskError=tf.split(LSQ, [1,1], 3)\n",
    "#loss=1*tf.reduce_mean(OutError)+0.1*tf.reduce_mean(MaskError)\n",
    "loss=tf.reduce_mean(LSQ, name='error')\n",
    "l2_loss = tf.losses.get_regularization_loss()\n",
    "#loss=loss+l2_loss/1000\n",
    "\n",
    "train_op=tf.train.AdamOptimizer(learning_rate=lr, name='trainer').minimize(loss)\n",
    "\n",
    "tf.summary.scalar('loss', loss)\n",
    "tf.summary.image('input', tf.reshape(x[:,:,:,0], [-1, x_dim, y_dim,1]), 3)\n",
    "tf.summary.image('standard', tf.reshape(y[:,:,:,0], [-1, x_dim, y_dim,1]) , 3)\n",
    "tf.summary.image('outline', tf.reshape(probs[:,:,:,0], [-1, x_dim, y_dim,1]), 3)\n",
    "\n",
    "\n",
    "\n",
    "merge = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shutil' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e49444f3451b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./logs/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m123456\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_to_keep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'shutil' is not defined"
     ]
    }
   ],
   "source": [
    "shutil.rmtree('./logs/')\n",
    "tf.set_random_seed(123456)\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(max_to_keep=50)\n",
    "test_writer = tf.summary.FileWriter('./logs/170/test')\n",
    "current_best=[0,1000000000]\n",
    "\n",
    "\n",
    "learning_rates=[0.000195, 0.0001, 0.00002, 0.00001]\n",
    "learning_rate_steps=[500,1500, 2200, 6000]\n",
    "current_step=0\n",
    "for lrate, lrs in zip(learning_rates, learning_rate_steps):\n",
    "    for i in range(current_step, lrs):\n",
    "        idx=np.random.choice(train_data.shape[0], replace=False, size=[15])\n",
    "        cur_train=train_data[idx,:,:,:]+np.random.uniform(-0.3, 0.3, 1)\n",
    "        cur_truth=train_truth[idx,:,:]\n",
    "        _, results, losses=sess.run([train_op,  probs, loss], feed_dict={x:cur_train, y:cur_truth, lr:lrate})\n",
    "        \n",
    "        #utils.plot_3x1(cur_train[0,:,:,0], cur_truth[0,:,:,0], results[0,:,:,0])\n",
    "        #plt.show()\n",
    "        \n",
    "        #train_writer.add_summary(summary, i)\n",
    "        if (i%100==0):\n",
    "            print(i)\n",
    "            print(\"Training loss: \",losses)\n",
    "            #idx=np.random.choice(validation_data.shape[0], replace=False, size=[50])\n",
    "            idx=range(0,3, 1)\n",
    "            sub_validation_data=validation_data[idx, :,:,:]\n",
    "            sub_validation_truth=validation_truth[idx, :,:]\n",
    "            summary, results, losses=sess.run([merge, probs, loss], feed_dict={x:sub_validation_data, y:sub_validation_truth})\n",
    "            test_writer.add_summary(summary, i)\n",
    "            print(results.shape)\n",
    "            print(\"Validation loss: \",losses)\n",
    "            for ti in range (0,3):\n",
    "                utils.plot_3x1(sub_validation_data[ti,:,:,0], sub_validation_truth[ti,:,:,0], results[ti,:,:,0])\n",
    "                plt.show()\n",
    "            saver.save(sess, data+'NewModels/Model'+str(i))\n",
    "    current_step=lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-35879278f7ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Data/NewModels/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'saved_3100.pb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "tf.train.write_graph(sess.graph_def, 'Data/NewModels/', 'saved_3100.pb', as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=np.random.choice(validation_data.shape[0], replace=False, size=[25])\n",
    "cur_train=train_data[idx,:,:,:]\n",
    "cur_truth=train_truth[idx,:,:]\n",
    "results, losses=sess.run([probs, loss], feed_dict={x:cur_train, y:cur_truth, lr:lrate})\n",
    "\n",
    "ids=4\n",
    "utils.plot_3x1(cur_train[ids,:,:,0], cur_truth[ids,:,:,0], results[ids,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process new datafiles using trained network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the network that works best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "data_model=data+'v1/Model'+str(4300)\n",
    "saver.restore(sess,data_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for processing a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(sess, file_path, save_dir):\n",
    "    file_data=skimage.external.tifffile.imread(file_path)\n",
    "    if (len(file_data.shape)==3):\n",
    "        file_data=np.reshape(file_data, [file_data.shape[0], x_dim, x_dim, 1])\n",
    "    else:\n",
    "        file_data=np.swapaxes(np.swapaxes(file_data,1,2), 2,3)\n",
    "    inference_data=file_data\n",
    "    \n",
    "    #true_test2_data=true_test2_data/np.std(true_test2_data)*0.02\n",
    "    #true_test2_data=true_test2_data-np.mean(true_test2_data)+0.55\n",
    "    #v1true_test2_data=(true_test2_data-.578)/.138+0.5\n",
    "    inference_data=(inference_data-mean)/std+0.5\n",
    "\n",
    "    channels=inference_data.shape[3]\n",
    "    num_images=inference_data.shape[0]\n",
    "    output=np.zeros([num_images,x_dim,x_dim,channels+1])\n",
    "    process_batch_size=20\n",
    "    print('Starting')\n",
    "    for t in range(0,num_images,process_batch_size):\n",
    "        endrng=np.min((t+process_batch_size,num_images))\n",
    "        inference_batch_data=inference_data[t:(t+process_batch_size), :,:,:]\n",
    "        results=sess.run(probs, feed_dict={x:inference_batch_data})\n",
    "        output[t:(t+process_batch_size),:,:,0:channels]=inference_batch_data[:,:,:,0:channels]\n",
    "        output[t:(t+process_batch_size),:,:,channels]=results[:,:,:,0]\n",
    "    print('Done')\n",
    "    inference_data=0\n",
    "    output[:,:,:,0:channels]=file_data[:,:,:,0:channels]\n",
    "    output=np.swapaxes(np.swapaxes(output,3,2),2,1)\n",
    "    np.place(output, output<0, 0)\n",
    "    file_name=file_path.split('/')[-1]\n",
    "    print(output.dtype)\n",
    "    skimage.external.tifffile.imsave(save_dir+file_name, output.astype('float32'), imagej=True)\n",
    "    print('Written')\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "for f in glob.glob('/n/projects/smc/public/STN/19-6-3b_ali_level_fullsize/DeepLearn/Data/*.tif'):\n",
    "    output=process_file(sess, f, data+'v1/Output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids=4\n",
    "utils.plot_3x1(output[ids,0,:,:], output[ids,1,:,:], output[ids,1,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain using new data and best old model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(data+'NewModels/'):\n",
    "    os.rename(data+'NewModels/', data+'OldModels/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both=skimage.external.tifffile.imread(data+'Training_retrain_RotShift.tif')\n",
    "both=np.swapaxes(np.swapaxes(both,1,2), 2,3)\n",
    "\n",
    "x_dim=both.shape[1]\n",
    "y_dim=both.shape[2]\n",
    "channels=both.shape[3]-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open(data+'Network.txt')\n",
    "base_scaler=int(file.readline())\n",
    "baseline_noise=float(file.readline())\n",
    "x_dim=int(file.readline())\n",
    "mean=float(file.readline())\n",
    "std=float(file.readline())\n",
    "model=int(file.readline())\n",
    "channels=int(file.readline())\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=both[:,:,:,0:-2]/655350.0\n",
    "train_truth=both[:,:,:,[-2,-1]]\n",
    "\n",
    "train_data=(train_data-mean)/(std*1)+0.5\n",
    "train_truth[np.where(train_truth>0.1)]=1\n",
    "\n",
    "validation=skimage.external.tifffile.imread(data+'Validation_annotated.tif')\n",
    "validation=np.swapaxes(np.swapaxes(validation,1,2), 2,3)\n",
    "\n",
    "validation_data=validation[:,:,:,0:-2]/655350.0\n",
    "validation_data=(validation_data-mean)/(std*1)+0.5\n",
    "\n",
    "validation_truth=validation[:,:,:,[-2,-1]]\n",
    "validation_truth[np.where(validation_truth>0.1)]=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#data_model=data+'OldModels/Model'+str(model)\n",
    "data_model=data+'OldModels/Model'+str(1500)\n",
    "saver.restore(sess,data_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree('./logs/')\n",
    "tf.set_random_seed(123456)\n",
    "saver = tf.train.Saver(max_to_keep=50)\n",
    "#train_writer = tf.summary.FileWriter('./logs/1/train ', sess.graph)\n",
    "test_writer = tf.summary.FileWriter('./logs/170/test')\n",
    "current_best=[0,1000000000]\n",
    "\n",
    "\n",
    "learning_rates=[0.00005, 0.00002, 0.00002, 0.00001]\n",
    "learning_rate_steps=[500,1500, 2200, 2200]\n",
    "current_step=0\n",
    "for lrate, lrs in zip(learning_rates, learning_rate_steps):\n",
    "    for i in range(current_step, lrs):\n",
    "        idx=np.random.choice(train_data.shape[0], replace=False, size=[25])\n",
    "        cur_train=train_data[idx,:,:,:]+np.random.uniform(-baseline_noise, baseline_noise, 1)\n",
    "        cur_truth=train_truth[idx,:,:]\n",
    "        _, results, losses=sess.run([train_op,  probs, loss], feed_dict={x:cur_train, y:cur_truth, lr:lrate})\n",
    "        #train_writer.add_summary(summary, i)\n",
    "        if (i%100==0):\n",
    "            print(i)\n",
    "            print(\"Training loss: \",losses)\n",
    "            #idx=np.random.choice(validation_data.shape[0], replace=False, size=[50])\n",
    "            idx=range(0,3, 1)\n",
    "            sub_validation_data=validation_data[idx, :,:,:]\n",
    "            sub_validation_truth=validation_truth[idx, :,:]\n",
    "            summary, results, losses=sess.run([merge, probs, loss], feed_dict={x:sub_validation_data, y:sub_validation_truth})\n",
    "            test_writer.add_summary(summary, i)\n",
    "            print(results.shape)\n",
    "            print(\"Validation loss: \",losses)\n",
    "            if (losses<current_best[1]):\n",
    "                current_best=[i, losses]\n",
    "                file=open(data+'Network.txt', 'w')\n",
    "                file.write(str(base_scaler)+'\\n')\n",
    "                file.write(str(baseline_noise)+'\\n')\n",
    "                file.write(str(x_dim)+'\\n')\n",
    "                file.write(str(mean)+'\\n')\n",
    "                file.write(str(std)+'\\n')\n",
    "                file.write(str(i)+'\\n')\n",
    "                file.write(str(channels)+'\\n')\n",
    "                file.close()\n",
    "            for ti in range (0,3):\n",
    "                utils.plot_3x1(sub_validation_data[ti,:,:,0], results[ti,:,:,0], results[ti,:,:,1])\n",
    "                plt.show()\n",
    "            saver.save(sess, data+'NewModels/Model'+str(i))\n",
    "    current_step=lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
